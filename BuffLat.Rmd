---
title: "Buffalo bTB Testing Data Analysis"
author: "Andrew J K Conlan"
date: "10/10/2023"
bibliography: BuffaloBLCA.bib  
csl: chicago-author-date-16th-edition.csl
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_and_reshape,echo=FALSE, fig.path='Figs/',fig.width=5, fig.height=5,results='hide', warning=FALSE,message=FALSE}
if(!require('tidyverse')){install.packages('tidyverse');require('tidyverse')}

buffalo <- as_tibble(read.csv('Buffalo_testing.csv'))
mean_percent <- function(x){100*mean(x)}
naive_errorL <- function(x){100*(mean(x)-sqrt(mean(x)*(1-mean(x)))/sqrt(length(x))) }
naive_errorU <- function(x){100*(mean(x)+sqrt(mean(x)*(1-mean(x)))/sqrt(length(x))) }

non_zero <- unlist(buffalo %>% group_by(s_no) %>% summarise(SIT=sum(SIT_Result),SICCT=sum(SICCT_Result),DST=sum(DST_Result)) %>% filter(SIT>0 | SICCT>0 |  DST>0) %>% select(s_no))

buff_nz <- buffalo %>% filter(is.element(s_no,non_zero))

G=length(unique(buff_nz$s_no))
A=dim(buff_nz)[1]
T = 3
E=2^T

response_mask = expand.grid(c(0,1),c(0,1),c(0,1))

# All animals
# Encode test pattern for each animal
y=sapply(1:dim(buff_nz)[1],function(i){which(apply(response_mask,1,function(x,i){prod(i==x)},i = buff_nz[i,22:24])==1)})

latent_dat <- list(G=G,A=A,T=T,E=E,D=2,gind=as.numeric(factor(buff_nz$s_no)),
                   tind=c(1,1,2),
                   y=y,
                   ind=as.matrix(response_mask))

# Data set for logistic regression analysis

buff_df <- buffalo %>% mutate(milk_prod = !is.na(milk_prod),
                   lact_stage = str_sub(buffalo$lact_stage,1,1))

#Code no lactation as zero (replace NA values)
buff_df$lact_stage[is.na(buff_df$lact_stage)] = 0

# Aggregate lactation stages >= 5 to single level '5+'
buff_df <- buff_df %>% mutate(lact_stage = sapply(lact_stage,function(x){if(x < 5){return(as.character(x))}else{return('5+')}}))


# Ensure that lactation variable is treated as factor
buff_df <- buff_df %>% mutate(milk_prod=factor(milk_prod),lact_stage=factor(lact_stage))


```

# Data Set

The data set consists of the results of `r T` diagnostic tests for bovine Tuberculosis (bTB) for `r A` animals taken from `r G` groups for which there was at least one positive test result. The diagnostic threshold for SIT & SICCT was taken as $\geq 4$ mm  ($\Delta B$, $\Delta B - \Delta A$) and $\geq 2$ mm for DST ($\Delta DST$).

Figure 1 below compares the apparent group prevalence in each herd based on these `r T` diagnostic tests, point estimates along with naive 95% binomial confidence intervals.

```{r group_prevalence,echo=FALSE, fig.path='Figs/',fig.width=5, fig.height=5,results='hide', warning=FALSE,message=FALSE}

naive_error <- function(x){er<-sqrt(mean(x)*(1-mean(x)))/sqrt(length(x))
data.frame(y=100*mean(x),ymin=100*(mean(x)-er),ymax=100*(mean(x)+er))}

group_tests <- buff_nz %>% mutate(group = as.numeric(factor(buff_nz$s_no))) %>% select(group,anim_id,age,SIT_Result,SICCT_Result,DST_Result) %>%
  pivot_longer(4:6,names_to = 'Test')

ggplot(group_tests,aes(x=as.factor(group),y=as.numeric(value),col=Test)) + stat_summary(fun.data=naive_error,position=position_dodge(width=0.5)) + 
  ylab('Apparent Prevalence %') + 
  xlab('Group')

latent_table <- do.call(rbind,tapply(latent_dat$y,latent_dat$gind,function(x){hist(x,breaks=c(0:8),plot=F)$counts}))

write.csv(t(latent_table),'latent_table.csv')

```

# Walter-Hui Latent Class Model

The Walter-Hui latent class model provides a theoretical framework to estimate the sensitivity and specificity of competing diagnostic tests when samples are available from at least two populations with differing prevalence [@collins_estimation_2014; @hui_estimating_1980]. A key assumption of the Walter-Hui (WH) model is that of conditional independence between tests, i.e. the probability of a test $k$ being positive for individual ($i$), $P(T_{i,k} = 1)$ only depends on the latent (true) disease status of the individual ($D \in \{0,1\}$) and not the response of the other tests. Under this assumption the (conditional) probability of a positive test result given that an animal is infected ($D=1$) or disease free ($D=0$) can then be modelled by a single parameter for each test:

\[ P(T_{i,k} = 1 | D = 1) = a_{k} \]
\[ P(T_{i,k} = 1 | D = 0) = b_{k} \]

and the sensitivity of test $k$ will then simply be $a_{k}$ and the specificity will be $1-b_{k}$.

Following [@collins_estimation_2014; @dendukuri_modeling_2009], and to allow for an extension to model any conditional dependence between tests, we parameterise the model using a probit ($\Phi$) link function :

\[ P(T_{t,k} = 1 | D = 1) = \Phi(a_{t,1}) \]
\[ P(T_{t,k} = 1 | D = 0) = \Phi(a_{t,0}) \]

To ensure numerical stability we restrict the sensitivity parameters (on the probit scale) $a_{t,1}$ to the range  $[-8,8]$. 

A common issue with this class of models is that the likelihood can be symmetric under relabeling of the latent variable. This leads to a multi-modal posterior distribution where where two modes can provide an equivalent fit to the data corresponding to the situation where the true positive rate is greater than the false positive rate (TPR > FPR, $\Phi(a_{t,1}) > \Phi(a_{t,0})$) and the inverse where the FPR > TPR ($\Phi(a_{t,0}) > \Phi(a_{t,1})$). Given the performance of the SIT and SICCT tests in other contexts we consider the situation where the FPR > TPR to be biologically implausible and choose prior assumptions to force selection of this mode by restricting $a_{t,0}$ to the range [-8,-1] - equivalent to assuming that no tests have a specificity of $< 80%$. For this baseline model we further assume non-informative normal (0,1) distributions for $a_{t,1}, a_{t,0}$ and a beta $(1,1)$ distribution for the true within-herd prevalence (i.e. the latent or true prevalence).

The SIT and SICTT test results have an implicit dependence on each other in the sense that they are both calculated based on the magnitude of the observed reaction to bovine tuberculin. The comparison to avian tuberculin in the SICCT test is intended to raise specificity and reduce sensitivity. A biological dependence between the tests â€“ implies - but does not guarantee that there will be a  statistical association between the results of the two tests strong enough to violate the assumption of conditional independence. If the correlation between (true) infection status and the results of each test result is large compared to the correlation between the test results themselves then the conditional dependence will not affect the parameter estimates and the test results can effectively be treated as (statistically) independent. @dendukuri_modeling_2009 proposed a method for testing whether the assumption of conditional dependence is satisfied by calculating the pairwise probability of agreement between each pair of diagnostic tests ($k,k'$):

\[ \alpha_{k,k'} = \frac{\sum_{i=1}^{N} T_{i,k} T_{i,k'} - (1-T_{i,k}) (1-T_{i,k'})}{N} \]

Any systematic differences between the observed ($\alpha_{k,k'}$) and expected values from the estimated model ($\alpha^{*}_{k,k'}$) would imply a violation of the assumption of conditional independence. We can use draws from the posterior predictive distribution of $\alpha^{*}_{k,k'}$ for our fitted model to form a posterior predictive p-value [@gelman_bayesian_2014]:

\[ P(\alpha^{*}_{k,k'} > \alpha_{k,k'}) \]

If the model fits well, the value of $P(\alpha^{*}_{k,k'} > \alpha_{k,k'})$ is expected to be close to $0.5$, with extreme values close to $0$ or $1$ indicating a lack of fit. 

As a further exploration of this issue we suggest an alternative model to account for potential conditional dependence between the SIT and SICCT tests. We propose that the sensitivity and specificity of the two tests can be interpreted as points on a shared receiver-operator-characteristic (ROC) curve. This is equivalent to a reparameterisation of the baseline model to use an assumed functional form that links the parameters for these two tests. 

Specifically, we use a normal model for the ROC curve [@irwin_lognormal_nodate] which arises from assuming that the false and true positive rates for each test have equal variance $\sigma_{g}$. The parameter $\sigma_{g}$ defines the shape of the ROC curve and the relationship between sensitivity and specificity for a group of tests $g$. Using this functional form we can write the conditional probabilities of a positive result as:

\[ P(T_{t,k} = 1 | D = 0) = \Phi(a_{t,0}) \]
\[ P(T_{t,k} = 1 | D = 1) = \Phi(\Phi^{-1}(a_{t,0}) + i_{g} \sigma_{g}) \]

Where we have two groups $g={1,2}$ with a shared parameter ($\sigma_{1}$) for the SIT and SICCT tests constraining them to lie on a shared ROC curve and a separate parameter $\sigma_{2}$ for the DST. For brevity, we will thus refer to this as the WHROC model.

For the whROC model we restrict the range of $\sigma_{g}$ to $[0,8]$ and use an indicator variable $i_g$ to choose between the default  (TPR > FPR; $i_g = 1$) and alternative prior assumptions (FPR > TPR; $i_g = -1$). As well as allowing us to explicitly model the potential dependence between SIT and SICCT tests, we can also illustrate the parameter estimates that would be obtained by specifying the alternative prior (FPR > TPR;$i_g = -1$).

Again, we assume normal(0,1) priors for both $a_{t,0}$ and $\sigma_g$ and a beta(1,1) prior for the (true or latent) within-herd prevalence. For the SIT/SICCT tests, and in common with the prior assumptions for the baseline model above we set $i_{g}=1$ corresponding to the assumption that both tests have a TPR > FPR.  Thus we fit a total of three models corresponding to the baseline WH model and the alternative WHROC model with the default and alternative prior assumptions:

* Model 1: Baseline WH model 
* Model 2: WHROC model SIT, SICCT (Default Prior, $i_{g}=1$), DST (Default Prior, $i_{g}=1$)
* Model 3: WHROC model SIT, SICCT (Alternative Prior, $i_{g}=-1$), DST (Alternative Prior, $i_{g}=-1$)

The models were implemented using stan and estimated using Hamiltonian MCMC [@carpenter_stan_2017]. Convergence was assessed through visual inspection of the chains and standard diagnostic statistics ($\hat{R}=1$ for all parameters after $4,000$ iterations for 8 chains). Convergence was assessed through visual inspection of the chains and standard diagnostic statistics ($\hat{R}=1$ for all parameters after $4,000$ iterations for 8 chains). We used leave-one-out (LOO) cross-validation [@vehtari_practical_2017] for model comparison and selection. The expected log pointwise predictive density: $\hat{elpd}$, which measures the predictive accuracy of the model when a single observation is dropped out, was estimated by Pareto smoothed importance sampling (PSIS-LOO) [https://mc-stan.org/loo]. The difference ($\Delta \hat{elpd}$) between $\hat{elpd}$ for alternative models fitted to the same data provides a measure of their relative predictive accuracy. The standard error on the difference gives a measure of uncertainty. Standard errors comparable to the magnitude of the difference ($\Delta \hat{elpd}$) suggest that the relative predictive accuracy of the two models is indistinguishable. 

\newpage

```{r fit_walter_hui,echo=FALSE, fig.path='Figs/',fig.width=5, fig.height=5,results='hide', cache=TRUE, warning=FALSE,message=FALSE}
if(!require(rstan)){install.packages('rstan');require('rstan')}
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

fit_wh <- stan(file = 'WHProbitB.stan', data = latent_dat, chains = 8, control = list(adapt_delta = 0.8))
```

```{r fit_whROC,echo=FALSE, fig.path='Figs/',fig.width=5, fig.height=5,results='hide', cache=TRUE, warning=FALSE,message=FALSE}
latent_dat$direction = c(1,1)
fit_whROC <- stan(file = 'WHProbitB_ROC.stan', data = latent_dat, chains = 8, control = list(adapt_delta = 0.8))
latent_dat$direction = c(-1,-1)
fit_whROCALT <- stan(file = 'WHProbitB_ROC.stan', data = latent_dat, chains = 8, control = list(adapt_delta = 0.8))

```

### Posterior estimates

```{r estimates,echo=FALSE, fig.path='Figs/',fig.width=8, fig.height=8, warning=FALSE,message=FALSE}
if(!require('tidybayes')){install.packages('tidybayes');require('tidybayes')}
if(!require('patchwork')){install.packages('patchwork');require('patchwork')}

sensitivity_post <- fit_wh %>% gather_draws(sensitivity[t])
sensitivity_post <- sensitivity_post %>% ungroup() %>% mutate(t=c('SIT','SICCT','DST')[t],
                                                              Model='WH')

tmp <- fit_whROC %>% gather_draws(sensitivity[t])
tmp <- tmp %>% ungroup() %>% mutate(t=c('SIT','SICCT','DST')[t],
                                                              Model='WHROC')

sensitivity_post <- sensitivity_post %>% bind_rows(tmp)

tmp <- fit_whROCALT %>% gather_draws(sensitivity[t])
tmp <- tmp %>% ungroup() %>% mutate(t=c('SIT','SICCT','DST')[t],
                                                              Model='WHROC Alt')

sensitivity_post <- sensitivity_post %>% bind_rows(tmp)


specificity_post <- fit_wh %>% gather_draws(specificity[t])
specificity_post <- specificity_post %>% ungroup() %>% mutate(t=c('SIT','SICCT','DST')[t],
                                                              Model='WH')

tmp <- fit_whROC %>% gather_draws(specificity[t])
tmp <- tmp %>% ungroup() %>% mutate(t=c('SIT','SICCT','DST')[t],
                                                              Model='WHROC')

specificity_post  <- specificity_post %>% bind_rows(tmp)

tmp <- fit_whROCALT %>% gather_draws(specificity[t])
tmp <- tmp %>% ungroup() %>% mutate(t=c('SIT','SICCT','DST')[t],
                                                              Model='WHROC Alt')

specificity_post  <- specificity_post %>% bind_rows(tmp)

prevalence_post <- fit_wh %>% gather_draws(prevalence[g]) %>% mutate(Model='WH')

prevalence_post <- prevalence_post %>% bind_rows(
  fit_whROC %>% gather_draws(prevalence[g]) %>% mutate(Model='WHROC'))

prevalence_post <- prevalence_post %>% bind_rows(
  fit_whROCALT %>% gather_draws(prevalence[g]) %>% mutate(Model='WHROC Alt'))

#prob_inf <- fit_wh %>% gather_draws(post_I[a]) %>% summarise(post_m = mean(.value),post_std = sd(.value)) 

credible <- function(x){data.frame(y=median(x),ymin=quantile(x,0.025),ymax=quantile(x,0.975))}

#ggplot(sensitivity_post,aes(x=t,y=100*.value)) + stat_summary(fun.data=credible, position=position_dodge(width=0.5)) + 
#  coord_flip() + ylab('% Sensitivity') + xlab('') + scale_x_discrete(limits = rev(levels(factor(sensitivity_post$t)))) + labs(col='')+ylim(c(0,100))

#print(ggplot(specificity_post,aes(x=t,y=100*.value)) + stat_summary(fun.data=credible, position=position_dodge(width=0.5)) + coord_flip() + ylab('% Specificity') + xlab('') + scale_x_discrete(limits = rev(levels(factor(sensitivity_post$t)))) + labs(col='')+ylim(c(0,100)))


p1 = ggplot(sensitivity_post %>% filter(Model=='WH'),
            aes(x=100*.value,fill=t)) + geom_density(alpha=0.25) + 
  xlab('% Sensitivity') + ylab('') + guides(fill=guide_legend(title=''))
p2 = ggplot(specificity_post %>% filter(Model=='WH'),
            aes(x=100*.value,fill=t)) + geom_density(alpha=0.25) + 
  xlab('% Specificity') + ylab('')  + guides(fill=guide_legend(title=''))
  

obs_prev <- group_tests %>% group_by(group,Test) %>% summarise(reactors=100*mean(value))
obs_prev <- obs_prev %>% mutate(Test = sapply(strsplit(Test,split='_'),function(x){x[1]}))

p3 = ggplot(prevalence_post %>% filter(Model=='WH'),
            aes(x=g,y=100*.value,shape='True')) + stat_summary(fun.data=credible, position=position_dodge(width=1)) + coord_flip()  + ylab('% Positive') + xlab('Herd') + scale_x_discrete(limits = rev(levels(prevalence_post$g)))+
geom_point(obs_prev,
             mapping=aes(x=group,
                         y=reactors,col=Test),size=2)+ labs(col='',shape='') + theme(legend.position="bottom")

print(p3 + (p1/p2 + plot_layout(guides = "collect")))

post_table <-sensitivity_post %>% bind_rows(specificity_post) %>% group_by(.variable,t,Model) %>% summarise(m=100*median(.value),l=100*quantile(.value,0.025),u=100*quantile(.value,0.975))

print(post_table)

```

### Comparison of model estimates for WH and WHROC models (default prior assumption)

```{r prior_comparison,echo=FALSE, fig.path='Figs/',fig.width=8, fig.height=8,results='hide', warning=FALSE,message=FALSE}

p1 = ggplot(sensitivity_post %>% filter(Model != 'WHROC Alt'),
            aes(x=100*.value,fill=t,linetype=Model)) + geom_density(alpha=0.25) + 
  xlab('% Sensitivity') + ylab('') + guides(fill=guide_legend(title='')) #+ facet_wrap(~Model,scales='free')
p2 = ggplot(specificity_post  %>% filter(Model != 'WHROC Alt'),
            aes(x=100*.value,fill=t,linetype=Model)) + geom_density(alpha=0.25) + 
  xlab('% Specificity') + ylab('')  + guides(fill=guide_legend(title='')) #+ facet_wrap(~Model,scales='free')
  

obs_prev <- group_tests %>% group_by(group,Test) %>% summarise(reactors=100*mean(value))
obs_prev <- obs_prev %>% mutate(Test = sapply(strsplit(Test,split='_'),function(x){x[1]}))

p3 = ggplot(prevalence_post  %>% filter(Model != 'WHROC Alt'),
            aes(x=g,y=100*.value,linetype=Model)) + stat_summary(fun.data=credible, position=position_dodge(width=1)) + coord_flip()  + ylab('% True Prevalence') + xlab('Herd') + scale_x_discrete(limits = rev(levels(prevalence_post$g)))+ labs(col='',shape='',linetype='') + theme(legend.position="bottom")


print(p3 + (p1/p2 + plot_layout(guides = "collect")))

```


### Comparison of parameter estimates for default (TPR > FPR) and alternative (FPR > TPR) prior assumptions (whROC model)
```{r prior_comparison_altROC,echo=FALSE, fig.path='Figs/',fig.width=8, fig.height=8,results='hide', warning=FALSE,message=FALSE}

p1 = ggplot(sensitivity_post %>% filter(Model != 'WH'),
            aes(x=100*.value,fill=t)) + geom_density(alpha=0.25) + 
  xlab('% Sensitivity') + ylab('') + guides(fill=guide_legend(title='')) + facet_grid(rows=vars(Model),scales='free_y')
p2 = ggplot(specificity_post  %>% filter(Model != 'WH'),
            aes(x=100*.value,fill=t)) + geom_density(alpha=0.25) + 
  xlab('% Specificity') + ylab('')  + guides(fill=guide_legend(title='')) + facet_grid(rows=vars(Model),scales='free_y')
  

obs_prev <- group_tests %>% group_by(group,Test) %>% summarise(reactors=100*mean(value))
obs_prev <- obs_prev %>% mutate(Test = sapply(strsplit(Test,split='_'),function(x){x[1]}))

p3 = ggplot(prevalence_post  %>% filter(Model != 'WH'),
            aes(x=g,y=100*.value,linetype=Model)) + stat_summary(fun.data=credible, position=position_dodge(width=1)) + coord_flip()  + ylab('% True Prevalence') + xlab('Herd') + scale_x_discrete(limits = rev(levels(prevalence_post$g)))+ labs(col='',shape='',linetype='') + theme(legend.position="bottom")


print(p3 + (p1/p2 + plot_layout(guides = "collect")))

```


## Posterior Predictive Checks of Model Fit

Model fit was assessed through two sets of posterior predictive checks: the apparent prevalence and the pairwise probability of agreement between each pair of diagnostic tests ($k,k'$) in each group (unaffected, affected).

### Apparent Prevalance

The fitted model demonstrates an excellent agreement with the apparent prevalence across all infected and uninfected herds. All of the observed values lie within the 95% posterior predictive intervals of the estimated model.

#### WH model

```{r apparentpp,echo=FALSE, fig.path='Figs/',fig.width=8, fig.height=8,results='hide', warning=FALSE,message=FALSE}
if(!require('parallel')){install.packages('parallel');require(parallel)}
simulate_WHA <- function(latent_dat,fit_wh)
{
  G = latent_dat$G
  A = latent_dat$A
  T = latent_dat$T
  E = latent_dat$E
  
  ind = latent_dat$ind
  
  seed=round(runif(1,-.Machine$integer.max,.Machine$integer.max))
  
  post <- fit_wh %>% spread_draws(a0[t],a1[t],n=1,seed=seed) 
  
  a0 = post$a0
  a1 = post$a1
  
  post <- fit_wh %>% spread_draws(prevalence[g],n=1,seed=seed) 
  
  prevalence = post$prevalence
  
  oot = numeric(A)
  
  for(a in 1:A)
  {
  
  p = numeric(E);
  for(e in 1:E)
  {
    s_prod = 1;
    p_prod = 1;
    
    for(t in 1:T)
    {
      s_prod = s_prod*(ind[e,t] * pnorm(a1[t]) + (1-ind[e,t]) * (1-pnorm(a1[t])));
      p_prod = p_prod*(ind[e,t] * pnorm(a0[t]) + (1-ind[e,t]) * (1-pnorm(a0[t])));
    }
    
      p[e] = prevalence[latent_dat$gind[a]] * (s_prod) + (1-prevalence[latent_dat$gind[a]]) * (p_prod);
    
  }
  oot[a] =  which(rmultinom(1,1,p)==1)
  }
return(oot)
}

pred<-mclapply(1:100,function(x){simulate_WHA(latent_dat,fit_wh)})

calculate_prevalence = function(y,latent_dat)
{
  pred <- as_tibble(data.frame(group=latent_dat$gind,latent_dat$ind[y,]))
  pred <- pred %>% group_by(group) %>% summarise(SIT=sum(Var1), SICCT=sum(Var2),DST=sum(Var3))
return(pred %>% pivot_longer(-group))
}

pp_prevalance <- do.call(rbind,lapply(pred,function(y){calculate_prevalence(y,latent_dat)}))

obs_reactors <- group_tests %>% group_by(group,Test) %>% summarise(value=sum(value))

obs_reactors <- obs_reactors %>% mutate(Test = str_remove(Test,'_Result'))

ggplot(pp_prevalance,aes(x=factor(name),y=value)) + 
  facet_wrap(~as.factor(group)) + coord_flip() + geom_violin() +
  geom_point(obs_reactors,
             mapping=aes(x=factor(Test),y=value),col='red') + 
  xlab('Diagnostic Test') +  ylab('Number of Positive Tests')

```

#### WH ROC model (default prior)

```{r apparentpp_ROC,echo=FALSE, fig.path='Figs/',fig.width=8, fig.height=8,results='hide', warning=FALSE,message=FALSE}

simulate_WHA <- function(latent_dat,fit_wh)
{
  G = latent_dat$G
  A = latent_dat$A
  T = latent_dat$T
  E = latent_dat$E
  D = latent_dat$D
  
  gind = latent_dat$gind
  tind = latent_dat$tind
  ind = latent_dat$ind
  direction = latent_dat$direction

  seed=round(runif(1,-.Machine$integer.max,.Machine$integer.max))
  
  post <- fit_wh %>% spread_draws(sensitivity[t],specificity[t],ndraws=1,seed=seed) 
  
  sensitivity = post$sensitivity
  specificity = post$specificity
  
  post <- fit_wh %>% spread_draws(prevalence[g],ndraws=1,seed=seed) 
  
  prevalence = post$prevalence
  
  oot = numeric(A)
  
   for(a in 1:A){
   p = numeric(E);

    for(e in 1:E)
    {
      s_prod = 1;
       p_prod = 1;
      
      for(t in 1:T)
      {
      s_prod = s_prod*(ind[e,t] * sensitivity[t] + (1-ind[e,t]) * (1-sensitivity[t]));
      p_prod = p_prod*(ind[e,t] * (1-specificity[t]) + (1-ind[e,t]) * specificity[t]);
      }
      
      p[e] = (prevalence[gind[a]] * (s_prod) + (1-prevalence[gind[a]]) * (p_prod));

    }
   
    oot[a] =  which(rmultinom(1,1,p)==1)
    
  }
  
return(oot)
}


latent_dat$direction = c(1,1)

predROC<-mclapply(1:100,function(x){simulate_WHA(latent_dat,fit_whROC)})

calculate_prevalence = function(y,latent_dat)
{
  pred <- as_tibble(data.frame(group=latent_dat$gind,latent_dat$ind[y,]))
  pred <- pred %>% group_by(group) %>% summarise(SIT=sum(Var1), SICCT=sum(Var2),DST=sum(Var3))
return(pred %>% pivot_longer(-group))
}

pp_prevalance <- do.call(rbind,lapply(predROC,function(y){calculate_prevalence(y,latent_dat)}))

obs_reactors <- group_tests %>% group_by(group,Test) %>% summarise(value=sum(value))

obs_reactors <- obs_reactors %>% mutate(Test = str_remove(Test,'_Result'))

ggplot(pp_prevalance,aes(x=factor(name),y=value)) + 
  facet_wrap(~as.factor(group)) + coord_flip() + geom_violin() +
  geom_point(obs_reactors,
             mapping=aes(x=factor(Test),y=value),col='red') + 
  xlab('Diagnostic Test') +  ylab('Number Positive') 

```


#### WH ROC model (default prior)

```{r apparentpp_ROCALT,echo=FALSE, fig.path='Figs/',fig.width=8, fig.height=8,results='hide', warning=FALSE,message=FALSE}

simulate_WHA <- function(latent_dat,fit_wh)
{
  G = latent_dat$G
  A = latent_dat$A
  T = latent_dat$T
  E = latent_dat$E
  D = latent_dat$D
  
  gind = latent_dat$gind
  tind = latent_dat$tind
  ind = latent_dat$ind
  direction = latent_dat$direction

  seed=round(runif(1,-.Machine$integer.max,.Machine$integer.max))
  
  post <- fit_wh %>% spread_draws(sensitivity[t],specificity[t],ndraws=1,seed=seed) 
  
  sensitivity = post$sensitivity
  specificity = post$specificity
  
  post <- fit_wh %>% spread_draws(prevalence[g],ndraws=1,seed=seed) 
  
  prevalence = post$prevalence
  
  oot = numeric(A)
  
   for(a in 1:A){
   p = numeric(E);

    for(e in 1:E)
    {
      s_prod = 1;
       p_prod = 1;
      
      for(t in 1:T)
      {
      s_prod = s_prod*(ind[e,t] * sensitivity[t] + (1-ind[e,t]) * (1-sensitivity[t]));
      p_prod = p_prod*(ind[e,t] * (1-specificity[t]) + (1-ind[e,t]) * specificity[t]);
      }
      
      p[e] = (prevalence[gind[a]] * (s_prod) + (1-prevalence[gind[a]]) * (p_prod));

    }
   
    oot[a] =  which(rmultinom(1,1,p)==1)
    
  }
  
return(oot)
}


latent_dat$direction = c(-1,-1)

predROCALT<-mclapply(1:100,function(x){simulate_WHA(latent_dat,fit_whROCALT)})

calculate_prevalence = function(y,latent_dat)
{
  pred <- as_tibble(data.frame(group=latent_dat$gind,latent_dat$ind[y,]))
  pred <- pred %>% group_by(group) %>% summarise(SIT=sum(Var1), SICCT=sum(Var2),DST=sum(Var3))
return(pred %>% pivot_longer(-group))
}

pp_prevalance <- do.call(rbind,lapply(pred,function(y){calculate_prevalence(y,latent_dat)}))

obs_reactors <- group_tests %>% group_by(group,Test) %>% summarise(value=sum(value))

obs_reactors <- obs_reactors %>% mutate(Test = str_remove(Test,'_Result'))

ggplot(pp_prevalance,aes(x=factor(name),y=value)) + 
  facet_wrap(~as.factor(group)) + coord_flip() + geom_violin() +
  geom_point(obs_reactors,
             mapping=aes(x=factor(Test),y=value),col='red') + 
  xlab('Diagnostic Test') +  ylab('Number Positive') 

```


### Pairwise probability of agreement between tests

#### WH Model 

```{r ppp_values, fig.path='Figs/',fig.width=5, fig.height=5, echo=FALSE, warning=FALSE,message=FALSE}

calculate_alpha = function(y,latent_dat)
{
  pred2 <- as_tibble(data.frame(group=latent_dat$gind,latent_dat$ind[y,]))
   
  out <- tibble(pair='0-1',alpha=NA)[-1,]
  
  for(t in 1:(latent_dat$T-1))
  {
    for(k in (t+1):(latent_dat$T))
    {
      out <- out %>% bind_rows(tibble(pair=paste(t,'+',k),alpha=mean(unlist(pred2[,t+1]*pred2[,k+1] - (1-pred2[,t+1])*(1-pred2[,k+1])))))
     }
  }
  return(out)
}

pp_alpha <- do.call(rbind,lapply(pred,function(y){calculate_alpha(y,latent_dat)}))

obs_alpha = calculate_alpha(y,latent_dat)

ggplot(pp_alpha,aes(x=as.factor(pair),y=alpha-obs_alpha$alpha)) + 
  geom_violin() + ylab('Observed - Expected') + xlab('Interaction')
 
obs_alphaV <- obs_alpha$alpha
names(obs_alphaV) <- obs_alpha$pair
 
# Predictive p-values
pp_alpha <- pp_alpha %>% mutate(obs_val=obs_alphaV[pair]) 
 
print(pp_alpha %>% group_by(pair) %>% summarise(mean(alpha > obs_val)))
```

#### ROC model (Default Priors)

```{r ppp_valuesROC, fig.path='Figs/',fig.width=5, fig.height=5, echo=FALSE,warning=FALSE,message=FALSE}

pp_alpha <- do.call(rbind,lapply(predROC,function(y){calculate_alpha(y,latent_dat)}))

obs_alpha = calculate_alpha(y,latent_dat)

ggplot(pp_alpha,aes(x=as.factor(pair),y=alpha-obs_alpha$alpha)) + 
  geom_violin() + ylab('Observed - Expected') + xlab('Interaction')
 
obs_alphaV <- obs_alpha$alpha
names(obs_alphaV) <- obs_alpha$pair
 
# Predictive p-values
pp_alpha <- pp_alpha %>% mutate(obs_val=obs_alphaV[pair]) 
 
print(pp_alpha %>% group_by(pair) %>% summarise(mean(alpha > obs_val)))
```

#### ROC model (Alternative Priors)

```{r ppp_valuesROCALT, fig.path='Figs/',fig.width=5, fig.height=5, echo=FALSE,warning=FALSE,message=FALSE}

pp_alpha <- do.call(rbind,lapply(predROCALT,function(y){calculate_alpha(y,latent_dat)}))

obs_alpha = calculate_alpha(y,latent_dat)

ggplot(pp_alpha,aes(x=as.factor(pair),y=alpha-obs_alpha$alpha)) + 
  geom_violin() + ylab('Observed - Expected') + xlab('Interaction')
 
obs_alphaV <- obs_alpha$alpha
names(obs_alphaV) <- obs_alpha$pair
 
# Predictive p-values
pp_alpha <- pp_alpha %>% mutate(obs_val=obs_alphaV[pair]) 
 
print(pp_alpha %>% group_by(pair) %>% summarise(mean(alpha > obs_val)))
```

### Leave-one-out (LOO) cross validation checks

##### WH Model
```{r loo,echo=FALSE, fig.path='Figs/',fig.width=5, fig.height=5, warning=FALSE,message=FALSE}
require(loo)
log_lik_wh   <- extract_log_lik(fit_wh  ,c('log_lik'),merge_chains=FALSE)
loo_wh   <- loo(log_lik_wh  , r_eff=relative_eff(exp(log_lik_wh  )),cores=4,save_psis = TRUE)
print(loo_wh)
plot(loo_wh)
```
##### WHROC Model
```{r loo_WHROC,echo=FALSE, fig.path='Figs/',fig.width=5, fig.height=5, warning=FALSE,message=FALSE}
require(loo)
log_lik_whROC   <- extract_log_lik(fit_whROC  ,c('log_lik'),merge_chains=FALSE)
loo_whROC   <- loo(log_lik_whROC  , r_eff=relative_eff(exp(log_lik_whROC  )),cores=4,save_psis = TRUE)
print(loo_whROC)
plot(loo_whROC)
```

```{r loo_WHROCALT,echo=FALSE, fig.path='Figs/',fig.width=5, fig.height=5, warning=FALSE,message=FALSE}

log_lik_whROCALT   <- extract_log_lik(fit_whROCALT  ,c('log_lik'),merge_chains=FALSE)
loo_whROCALT   <- loo(log_lik_whROCALT  , r_eff=relative_eff(exp(log_lik_whROCALT  )),cores=4,save_psis = TRUE)
print(loo_whROCALT)
plot(loo_whROCALT)
```

#### Model Comparison

```{r loo_model_compare,echo=FALSE, fig.path='Figs/',fig.width=5, fig.height=5, warning=FALSE,message=FALSE}
loo_compare(loo_wh,loo_whROC,loo_whROCALT)
```

# References